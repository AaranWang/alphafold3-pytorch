from functools import partial

import torch
import torch.nn.functional as F
from torch.nn import Module, Linear, Sequential

from alphafold3_pytorch.typing import (
    Float,
    Int,
    Bool,
    typecheck
)

# constants

LinearNoBias = partial(Linear, bias = False)

# helper functions

def exists(v):
    return v is not None

def default(v, d):
    return v if exists(v) else d

# classic feedforward, SwiGLU variant
# they name this 'transition' in their paper
# Algorithm 11

class SwiGLU(Module):
    @typecheck
    def forward(
        self,
        x: Float['b n d']
    ) -> Float['b n (d//2)']:

        x, gates = x.chunk(2, dim = -1)
        return F.silu(gates) * x

class Transition(Module):
    def __init__(
        self,
        *,
        dim,
        expansion_factor = 4
    ):
        super().__init__()
        dim_inner = int(dim * expansion_factor)

        self.ff = Sequential(
            LinearNoBias(dim, dim_inner * 2),
            SwiGLU(),
            LinearNoBias(dim_inner, dim)
        )

    @typecheck
    def forward(
        self,
        x: Float['b n d']
    ) -> Float['b n d']:

        return self.ff(x)

# main class

class Alphafold3(Module):
    def __init__(self):
        super().__init__()
